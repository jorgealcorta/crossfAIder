{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crossfAIder - pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Extract features for A and B using VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using Encodec by facebook https://huggingface.co/docs/transformers/model_doc/encodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import EncodecModel, AutoProcessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Encodec model capable of extracting audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jorge\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jorge\\.cache\\huggingface\\hub\\models--facebook--encodec_24khz. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Jorge\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load the EnCodec model\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encodec_features(audio_path):\n",
    "    waveform, sr = torchaudio.load(audio_path)  # Loads an audio file as a waveform tensor.\n",
    "    waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=processor.sampling_rate) # Ensures the sample rate is 24 kHz, as required by EnCodec\n",
    "\n",
    "    # Prepare input for EnCodec\n",
    "    inputs = processor(raw_audio=waveform, sampling_rate=processor.sampling_rate, return_tensors=\"pt\") # Converts the waveform into a PyTorch tensor for processing\n",
    "    \n",
    "    # Encode audio into latent representation\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"]) # Extracts audio_codes (the compressed latent features) and audio_scales (quantization scales)\n",
    "    \n",
    "    return encoder_outputs.audio_codes, encoder_outputs.audio_scales  # Encoded features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an interpolation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO try alternatives to Linear Interpolation:\n",
    "\n",
    "Spherical Interpolation (SLERP): More natural blending in latent space.\n",
    "\n",
    "Bezier Curves: Non-linear transitions for smoother effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Uses linear interpolation between Track A and Track B in 10 steps\n",
    "def interpolate_encodec_features(audio_codes_A, audio_codes_B, steps=10):\n",
    "    interpolations = []\n",
    "    # alpha varies from 0 to 1 (0 = Track A, 1 = Track B)\n",
    "    for alpha in np.linspace(0, 1, steps):\n",
    "        # Linear interpolation between both audio representations\n",
    "        interpolated_code = (1 - alpha) * audio_codes_A + alpha * audio_codes_B\n",
    "        interpolations.append(interpolated_code)\n",
    "\n",
    "    return interpolations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode back to waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_encodec_features(interpolated_codes):\n",
    "    \"\"\"Decodes interpolated EnCodec latent representations back to audio.\"\"\"\n",
    "    interpolated_audio = []\n",
    "    \n",
    "    for codes in interpolated_codes:\n",
    "        with torch.no_grad():\n",
    "            decoded_audio = model.decode(audio_codes=codes, audio_scales=None, padding_mask=None)\n",
    "        \n",
    "        interpolated_audio.append(decoded_audio.squeeze().cpu().numpy())\n",
    "    \n",
    "    return interpolated_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from Track A (end) and Track B (start)\n",
    "audio_codes_A, _ = extract_encodec_features(\"trackA_end.wav\")\n",
    "audio_codes_B, _ = extract_encodec_features(\"trackB_start.wav\")\n",
    "\n",
    "# Interpolate between the two tracks\n",
    "interpolated_codes = interpolate_encodec_features(audio_codes_A, audio_codes_B, steps=10)\n",
    "\n",
    "# Decode the interpolated representations back to audio\n",
    "audio_transitions = decode_encodec_features(interpolated_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save one of the transition steps as a file\n",
    "import soundfile as sf\n",
    "sf.write(\"transition.wav\", audio_transitions[5], samplerate=processor.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crossfaider_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
