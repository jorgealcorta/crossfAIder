{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crossfAIder - pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Extract features for A and B using VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using Encodec by facebook https://huggingface.co/docs/transformers/model_doc/encodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import EncodecModel, AutoProcessor\n",
    "import numpy as np\n",
    "import IPython\n",
    "import torchaudio.transforms as T\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Encodec model capable of extracting audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load the EnCodec model\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encodec_features(audio_path, duration=5, last=True):\n",
    "\n",
    "    # Load MP3 file using pydub\n",
    "    audio = AudioSegment.from_mp3(audio_path)\n",
    "\n",
    "    # Determine start time\n",
    "    start_time = (audio.duration_seconds - duration) if last else 0  # Last X seconds or first X seconds\n",
    "\n",
    "    # Extract the required segment\n",
    "    segment = audio[start_time * 1000 : (start_time + duration) * 1000]  # Convert to milliseconds\n",
    "\n",
    "    # Convert to MONO\n",
    "    segment = segment.set_channels(1)\n",
    "\n",
    "    # Convert to waveform tensor\n",
    "    samples = torch.tensor(segment.get_array_of_samples()).float()\n",
    "    waveform = samples / (2**15)  # Normalize (convert int16 to float)\n",
    "\n",
    "    # Ensure correct shape for EnCodec: (1, samples) instead of (samples, 1, 1)\n",
    "    waveform = waveform.unsqueeze(0)  # Add batch dimension → Shape (1, samples)\n",
    "\n",
    "    # Resample to EnCodec’s required sample rate (24 kHz)\n",
    "    waveform = T.Resample(orig_freq=audio.frame_rate, new_freq=processor.sampling_rate)(waveform)\n",
    "\n",
    "    # Ensure correct shape: (channels, samples) → EnCodec expects (1, samples)\n",
    "    waveform = waveform.squeeze(0)  # Remove batch dim → Now shape (1, samples)\n",
    "\n",
    "    # Prepare for EnCodec\n",
    "    inputs = processor(raw_audio=waveform, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "    # Encode to latent space\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n",
    "\n",
    "    return encoder_outputs.audio_codes  # Return only the audio codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an interpolation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO try alternatives to Linear Interpolation:\n",
    "\n",
    "Spherical Interpolation (SLERP): More natural blending in latent space.\n",
    "\n",
    "Bezier Curves: Non-linear transitions for smoother effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def interpolate_encodec_features(audio_codes_A, audio_codes_B, steps=10):\n",
    "    \n",
    "    interpolations = []\n",
    "    \n",
    "    for alpha in np.linspace(0, 1, steps):\n",
    "        # Ensure interpolation preserves shape (batch, streams, codebooks, frames)\n",
    "        interpolated_code = (1 - alpha) * audio_codes_A + alpha * audio_codes_B\n",
    "        interpolations.append(interpolated_code)\n",
    "    \n",
    "    # Stack into a tensor of shape (steps, batch, streams, codebooks, frames)\n",
    "    return torch.cat(interpolations, dim=0)  # Now it's a single tensor with batch dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode back to waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_encodec_features(interpolated_codes):\n",
    "    \"\"\"Decodes EnCodec latent codes into audio waveforms.\"\"\"\n",
    "    interpolated_audio = []\n",
    "    \n",
    "    for codes in interpolated_codes:\n",
    "        # Add batch dimension if not present\n",
    "        if len(codes.shape) == 3:  # [n_q, T] → [1, n_q, T]\n",
    "            codes = codes.unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get the scales - this is critical\n",
    "            # For EnCodec, scales are usually calculated during encoding\n",
    "            # Since we're not encoding, we need to create default scales\n",
    "            # The shape should match the codes: [batch_size, channels, T]\n",
    "            # or whatever shape your model expects\n",
    "            \n",
    "            # Option 1: Create default scales (all ones)\n",
    "            scales_shape = (codes.shape[0], 1, codes.shape[-1])  # [batch, 1, frames]\n",
    "            audio_scales = torch.ones(scales_shape, device=codes.device)\n",
    "            \n",
    "            # Decode with scales\n",
    "            decoded_audio = model.decode(\n",
    "                audio_codes=codes,\n",
    "                audio_scales=audio_scales,\n",
    "                padding_mask=None\n",
    "            )\n",
    "        \n",
    "        decoded_audio = decoded_audio.squeeze().cpu().numpy()\n",
    "        interpolated_audio.append(decoded_audio)\n",
    "    \n",
    "    return interpolated_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2, 375])\n",
      "torch.Size([1, 1, 2, 375])\n"
     ]
    }
   ],
   "source": [
    "# Extract last 5 seconds of Track A\n",
    "trackA_codes = extract_encodec_features(\"./tracks/trackA.mp3\", duration=5, last=True)\n",
    "# Extract first 5 seconds of Track B\n",
    "trackB_codes = extract_encodec_features(\"./tracks/trackB.mp3\", duration=5, last=True)\n",
    "\n",
    "print(trackA_codes.shape)\n",
    "print(trackB_codes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 2, 375])\n"
     ]
    }
   ],
   "source": [
    "# Generate interpolated features\n",
    "interpolated_codes = interpolate_encodec_features(trackA_codes, trackB_codes, steps=10)\n",
    "\n",
    "print(interpolated_codes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Decode back to audio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m audio_transitions = \u001b[43mdecode_encodec_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolated_codes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mdecode_encodec_features\u001b[39m\u001b[34m(interpolated_codes)\u001b[39m\n\u001b[32m     19\u001b[39m     audio_scales = torch.ones(scales_shape, device=codes.device)\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Decode with scales\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     decoded_audio = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43maudio_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43maudio_scales\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_scales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m decoded_audio = decoded_audio.squeeze().cpu().numpy()\n\u001b[32m     29\u001b[39m interpolated_audio.append(decoded_audio)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:747\u001b[39m, in \u001b[36mEncodecModel.decode\u001b[39m\u001b[34m(self, audio_codes, audio_scales, padding_mask, return_dict)\u001b[39m\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio_codes) != \u001b[32m1\u001b[39m:\n\u001b[32m    746\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected one frame, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(audio_codes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     audio_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_codes\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_scales\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    749\u001b[39m     decoded_frames = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:711\u001b[39m, in \u001b[36mEncodecModel._decode_frame\u001b[39m\u001b[34m(self, codes, scale)\u001b[39m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decode_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m, codes: torch.Tensor, scale: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m    710\u001b[39m     codes = codes.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.decoder(embeddings)\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:445\u001b[39m, in \u001b[36mEncodecResidualVectorQuantizer.decode\u001b[39m\u001b[34m(self, codes)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(codes):\n\u001b[32m    444\u001b[39m     layer = \u001b[38;5;28mself\u001b[39m.layers[i]\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     quantized = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m     quantized_out = quantized_out + quantized\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m quantized_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:401\u001b[39m, in \u001b[36mEncodecVectorQuantization.decode\u001b[39m\u001b[34m(self, embed_ind)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_ind):\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     quantize = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_ind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m     quantize = quantize.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m quantize\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:382\u001b[39m, in \u001b[36mEncodecEuclideanCodebook.decode\u001b[39m\u001b[34m(self, embed_ind)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_ind):\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     quantize = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m quantize\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Decode back to audio\n",
    "audio_transitions = decode_encodec_features(interpolated_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 5th interpolated step as an MP3 transition file\n",
    "save_as_mp3(audio_transitions[5], \"transition.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crossfaider_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
