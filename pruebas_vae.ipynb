{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSSFAIDER - First Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import EncodecModel, AutoProcessor\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Facebooks Encodec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\azict\\anaconda3\\envs\\crossfaider_env\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features of A's end and B's beginning using Encodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def extract_encodec_features(audio_path, duration=5, last=True):\n",
    "\n",
    "    # Load audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert to mono by averaging channels if multiple channels exist\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Determine the amount of samples (=duration) to keep\n",
    "    total_samples = waveform.shape[1]\n",
    "    samples_to_keep = int(duration * sample_rate)\n",
    "    \n",
    "    # Select the first or last duration seconds\n",
    "    if last:\n",
    "        start_sample = max(0, total_samples - samples_to_keep)\n",
    "    else:\n",
    "        start_sample = 0\n",
    "    \n",
    "    # Calculate the end position for the segment\n",
    "    end_sample = min(start_sample + samples_to_keep, total_samples)\n",
    "\n",
    "    # Slice the waveform between start_sample and end_sample\n",
    "    waveform = waveform[:, start_sample:end_sample]\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sample_rate != processor.sampling_rate:\n",
    "        waveform = T.Resample(orig_freq=sample_rate, new_freq=processor.sampling_rate)(waveform)\n",
    "    \n",
    "    # For compatibility with encodec processor, which expects a 1D waveform (just the sample values), we remove the channel dimension\n",
    "    waveform = waveform.squeeze(0)\n",
    "    \n",
    "    # Process for encodec\n",
    "    inputs = processor(raw_audio=waveform, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n",
    "    \n",
    "    # Encode audio features into compressed encodec features (audio_codes, audio_scales)\n",
    "    # audio_codes: quantized tokens that represent original audio in compressed form\n",
    "    # audio_scales: adjusts the loudness/amplitude of the encoded features\n",
    "    with torch.no_grad():\n",
    "        # input_values: preprocessed audio waveform in tensor form\n",
    "        # padding_mask: indicates which parts of the input are valid audio and which are padding\n",
    "        encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n",
    "    \n",
    "    return encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate between A and B features\n",
    "\n",
    "Each interpolation step generates a new version of the audio with a slightly different mix between Track A and Track B. These individual versions represent gradual transformation from A â†’ B.\n",
    "\n",
    "At alpha = 0.0, the audio is 100% Track A.\n",
    "At alpha = 1.0, the audio is 100% Track B.\n",
    "Intermediate values (e.g., alpha = 0.2, alpha = 0.4, etc.) create intermediate sounds.\n",
    "Each of these is saved as a separate .wav file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_encodec_features(features_A, features_B, steps=10):\n",
    "    # input: features - tuples of (audio_codes, audio_scales, padding_mask)\n",
    "    # returns list of tuples containing interpolated features\n",
    "    \n",
    "    audio_codes_A, audio_scales_A, padding_mask_A = features_A\n",
    "    audio_codes_B, audio_scales_B, padding_mask_B = features_B\n",
    "    \n",
    "    # Ensure that both feature sets have the same shape\n",
    "    if audio_codes_A.shape != audio_codes_B.shape:\n",
    "        raise ValueError(f\"Shape mismatch: A codes {audio_codes_A.shape}, B codes {audio_codes_B.shape}\")\n",
    "    \n",
    "    interpolations = []\n",
    "\n",
    "    # Generate steps evenly spaced, from values 0 to 1\n",
    "    # alpha represents the interpolation weight between A (alpha=0) and B (alpha=1)\n",
    "    for alpha in np.linspace(0, 1, steps):\n",
    "        # For discrete tokens, use probabilistic selection\n",
    "        # Create a random matrix with values between 0 and 1, matching the shape of audio_codes, where values<alpha are true (= take from audio_codes_B)\n",
    "        # For intermediate values, a random mix occurs\n",
    "        selection_mask = torch.rand_like(audio_codes_A.float()) < alpha\n",
    "        \n",
    "        # Create new tensor initialized with codes from A\n",
    "        interpolated_code = audio_codes_A.clone()\n",
    "        # Use the selection_mask to replace certain values with audio_codes_B\n",
    "        interpolated_code[selection_mask] = audio_codes_B[selection_mask]\n",
    "        \n",
    "        # Interpolate scales based on type\n",
    "        if isinstance(audio_scales_A, torch.Tensor) and isinstance(audio_scales_B, torch.Tensor):\n",
    "            # If they are tensors do weighted average\n",
    "            interpolated_scale = (1 - alpha) * audio_scales_A + alpha * audio_scales_B\n",
    "        else:\n",
    "            # If they are not tensors use A scales for alpha < 0.5, otherwise use B, hard switch\n",
    "            interpolated_scale = audio_scales_A if alpha < 0.5 else audio_scales_B\n",
    "        \n",
    "        # Use padding mask from A (they should be the same length anyway)\n",
    "        interpolations.append((interpolated_code, interpolated_scale, padding_mask_A))\n",
    "    \n",
    "    return interpolations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode EnCodec features into audio waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_encodec_features(interpolated_features):\n",
    "\n",
    "    #input: interpolated_features = list of tuples (audio_codes, audio_scales, padding_mask)\n",
    "    # returns List of decoded audio arrays\n",
    "\n",
    "    interpolated_audio = []\n",
    "    \n",
    "    for codes, scales, mask in interpolated_features:\n",
    "        with torch.no_grad():\n",
    "            # Ensure audio codes are integer type\n",
    "            codes = codes.long()\n",
    "            \n",
    "            # Decode\n",
    "            audio_output = model.decode(\n",
    "                audio_codes=codes,\n",
    "                audio_scales=scales,\n",
    "                padding_mask=mask\n",
    "            )\n",
    "            \n",
    "            # Extract audio values\n",
    "            audio_values = audio_output.audio_values\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            audio_np = audio_values.squeeze().cpu().numpy()\n",
    "            \n",
    "        interpolated_audio.append(audio_np)\n",
    "    \n",
    "    return interpolated_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio_files(interpolated_audio, sample_rate=24000, base_filename=\"interpolation\", save_full_transition=True):\n",
    "\n",
    "    audio_files = []\n",
    "    \n",
    "    # Process each individual interpolated segment\n",
    "    for i, audio in enumerate(interpolated_audio):\n",
    "        # Normalize audio to prevent clipping\n",
    "        if np.abs(audio).max() > 0.0:\n",
    "            audio = audio / np.abs(audio).max()\n",
    "        \n",
    "        filename = f\"{base_filename}_{i}.wav\"\n",
    "        write_wav(filename, sample_rate, audio.astype(np.float32))\n",
    "        audio_files.append(filename)\n",
    "        \n",
    "        print(f\"Saved {filename}\")\n",
    "    \n",
    "    # Save the full transition if requested\n",
    "    if save_full_transition and len(interpolated_audio) > 0:\n",
    "        # Concatenate all audio segments\n",
    "        full_transition = np.concatenate(interpolated_audio, axis=0)\n",
    "        \n",
    "        # Normalize the full transition\n",
    "        if np.abs(full_transition).max() > 0.0:\n",
    "            full_transition = full_transition / np.abs(full_transition).max()\n",
    "        \n",
    "        # Save as a single file\n",
    "        full_filename = f\"{base_filename}_full_transition.wav\"\n",
    "        write_wav(full_filename, sample_rate, full_transition.astype(np.float32))\n",
    "        audio_files.append(full_filename)\n",
    "        \n",
    "        print(f\"Saved full transition as {full_filename}\")\n",
    "    \n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interpolation_0.wav\n",
      "Saved interpolation_1.wav\n",
      "Saved interpolation_2.wav\n",
      "Saved interpolation_3.wav\n",
      "Saved interpolation_4.wav\n",
      "Saved full transition as interpolation_full_transition.wav\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "features_A = extract_encodec_features(\"track_A.mp3\", duration=10)\n",
    "features_B = extract_encodec_features(\"track_B.mp3\", duration=10, last=False)\n",
    "interpolated_features = interpolate_encodec_features(features_A, features_B, steps=5)\n",
    "interpolated_audio = decode_encodec_features(interpolated_features)\n",
    "audio_files = save_audio_files(interpolated_audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crossfaider_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
